
Python常见的坑----默认编码是ascii,不是unicode,无法解析Unicode（Python3版本可以）
                  解决的方法：通常在代码最前面加上两句  reload（sys）
                                                     sys.setdefaultencoding('utf-8')

登录的步骤
下面的方法采用requets库，可以百度参考下官方文档使用方法，也可以参考使用urllib和urllib2这两个库
import sys
import requests
from bs4 import beautifulSoup


-------获取session避免后续重复登录-------
s=requests.session()
s.trust_env=False     #设置忽略代理，最好还是将代理加上，因为有些网站不允许机器爬虫，此时就要设置好代理

def login():
    name='your xx'
    pwd='your xx'
    params={
            '里面的参数需要根据实际登录网页决定，可以输入错误的信息，
             然后看网页源码提示需要填充哪些信息'
            }
     login_url='-------------------'#登录的网页，具体查看你登录的form,不要被浏览器输入框的URL迷惑了哦
     
     res=s.post(login_url,params=params)  #采用session的post方式登录
     global cookie
     cookie=res.cookies  #获取cookie，后面获取网页源码需要使用，不保留的话就会要重新登录
     
     if res.status_code==200:  #返回200表示服务器返回页面成功  具体参数可以查看服务器返回状态码
          print（'successful connected'）
          if cookie is None or cookie['login_failLoginCount'] is None or cookie['login_failLoginCount']!='0': 
                print('fail login')
          else:
                print('succes login')
第二步：查找相关信息  当然也可以使用xpath,或者使用正则
def seek():
    base_url='----------'
    r=s.get(base_url,cookies=cookie)  #这里的s对象就是前面requests.session()实例化过来的，cookie也是前面登录获取的，用来避免重复登录
    if r.status_code==200:
        page_soup=BeautifulSoup(r.content.decode('utf-8','ignore'),'lxml')#用beautiful采用lxml解析网页
        if page_soup:
            info=page_soup.find('div',class='page mb20 aLr MT5')




1. 获取网页响应头的常用操作方法：
req=urllib2.Request('URL')
page=urllib2.urlopen(req)
page.info().getheaders('Server')  //此处要根据网页的具体关键字来填充

2.获得cookie的方法
a.手动获取，直接在浏览器中，按住F12然后刷新网页在Internet选项中去复制对应的cookie即可
b.通过获取相应头的方式来获取
  s=requests.session()
  res=s.post(login_url,params=params)  #采用session的post方式登录
  cookie=res.cookies  #获取cookie，后面获取网页源码需要使用，不保留的话就会要重新登录
   r=s.get(base_url,cookies=cookie)  #这里的s对象就是前面requests.session()实例化过来的，cookie也是前面登录获取的，用来避免重复登录
      
c.通过selenium登录网页，然后共享cookie给request库，（该方法得百度，还没尝试）


3.NTLK文本分析步骤总结：
    要点:
    1. selenium  取 body.text  或者 requests 取 返回的代码
    2. 了解 snownlp  方法用法   该模块可以识别哪些词语是积极的，哪些属于消极评价的
    #遍历记录 ,每条记录的结构示例如下:{"content":"这是我买的第四部手机","createDate":"2017-08-19 11:17","custId":150086,"custName":"Z***g","custNameStatus":3,"gradeCode":3,"id":4941161,"labelList":["分辨率高","画质细腻","系统流畅","照相给力","长久续航"],"msgReplyList":[{"id":870906,"isSystemAdmin":"1","isshow":"1","replyContent":"这么好用的手机，当然值得您一次次选择，有您的支持真好~","replyTime":"2017-08-19 17:29:06","replyerGradeName":"0","replyerId":969,"replyerName":"华为商城","replyerloginName":"华为商城"}],"productId":"738677717","remarkLevel":"好评","score":5}
    for record in record_list:
        positive_degree = SnowNLP(record["content"]).sentiments # 正向评价度,在0~1.0之间, 靠近0为负面评价,靠近1为正面评价
        if positive_degree < 0.5:
            negative_num += 1
            record["positive"] = 0 #消极
        else:
            positive_num += 1
            record["positive"] = 1 #积极
    3. 了解 jieba 分词模块的用法 该模块可以进行分词操作   在使用jieba 分词时,出现了"，"," 。", "！","我"等单个字的分词,也就是去停词的操作

    # 将字典转为列表,例如:[("画面",10), ("快",20)]
    word_list = list(word_dict.items())
    # 因为1个汉字不能表达足够的意思,所以过滤掉
    word_list = list(filter(lambda x: len(x[0]) > 1, word_list))
    # 对列表中按统计次数降序排列
    word_list.sort(key=lambda word_num : word_num[1], reverse=True)
    # 将列表中的前50个单词进行输出
    print(convert_code("根据(%s)统计,出现最多的50个关键词:" % record_field_name ))
    print(",".join([ x[0] for x in word_list[0:50]]))
    
    def convert_code(src_str):
    """对utf-8源码文件中的汉字串进行转码,否则在python2中出现乱码"""
    import sys
    if sys.version_info.major == 2:
        return unicode(src_str.decode("utf-8"))
    else:
        return src_str


