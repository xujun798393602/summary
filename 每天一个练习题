相关知识点文档参考：  
                 1.用户密码存储与Python示例
                 2.Hashing String with python

        每天一个练习题

1.在一个指定图片上加上一个数字0或者1

2.为APP应用生成激活码（或者优惠券码），使用Python生成200个激活码

3.将上面生成的200个激活码保存到Mysql关系型数据库和redis非关系型数据库中

4.在一个英文纯文本文件中，统计其中单词出现的个数

5.把指定目录中所有像素的图片转换成不大于iPhone5分辨率大小

6.一个目录放到很多日记，全是txt格式，统计每篇日记中最重要的词（）

7.一个HTML文件，找出里面的正文和所有连接（正则匹配的用法）

8.使用Python生成类似数字字母验证码的验证码图片

9.敏感词提示：建立一个敏感词文件，只要句子中出现文件库中的敏感词，就用**将敏感词替代

10.纯文本文件，student.txt  {"1":["张三，150100,102"]，“2”：[“李四”，120,130,100]}
   将上述内容写到student.xls文件中

习题2：
# _*_ coding:utf-8 _*_

import string,random

#激活码中的字符和数字
Activation_code=string.ascii_letters+string.digits

#获得四个字母和随机数的随机组合
def getRandom(string_active):
    return "".join(random.sample(string_active,4))

#group：每个验证码由几个随机码组成  下面格式表示又4个随机码组成验证码
#生成每个激活码有几组 eg: xxxxxx-xxxxxx-xxxxxx-xxxxxx
def concatenate(group,string_active):
    return "-".join([getRandom(string_active) for i in range(group)])

# 参数n --产生多少个验证码  group：每个验证码由几个随机码组成
def generate(n,group,string_active):
    return [concatenate(group,string_active) for i in range(n)]

print(generate(20,4,Activation_code))

if __name__=='__main__':
    print(generate(200,4,Activation_code))




习题4：
     步骤1：去掉标点符号  采用方式可以去停词或者直接用replace去掉，但是后者花费的时间应该会长一些
         2.切割分词，采用精确模式切分，因为全模式导致的重复词汇太多
    s='我们在北京，我们过得很好，很不错'
    s=s.replace(r'，',' ')
    print("/".join(jieba.cut(s,cut_all=False)))#切分句子
    b=jieba.analyse.textrank(string1, topK = 3, withWeight = True)  #获取权重最大的三个值
    print(b,'==========================')


习题6：
# _*_ coding:utf-8 _*_
import jieba
import jieba.posseg as pseg
import jieba.analyse

with open('Test.txt','r') as f: #读取文本
    string1=f.read()
    print (string1)
    print('===============')
    words=pseg.cut(string1)#进行分词
    result=''#记录最终结果的变量
    b=jieba.analyse.textrank(string1, topK = 3, withWeight = True)  #获取权重最大的三个值
    # for w in words:
    #     result +=str(w.word)+"/"+str(w.flag)  #加词性标注
    # print (result)
    
    
 习题9：
 #_*_ coding:utf-8 _*_

import jieba
import string

#eg:'你好，北京长城故宫。我喜欢吃大西瓜。哈哈哈' 
#    效果-----输入上述内容，把'哈哈哈'当成敏感词用'****'输出来显示


#读取当前敏感词文件中敏感词列表
def ReadSensitiveFile(File1='./sensitiveFile.txt'):
    f_list=[]
    with open(File1,'r+') as Fp:
        for words in Fp:
            f_list.append(words.rstrip('\n'))
    return f_list
 
#分词操作
def TokenWord(inputString=''):
    return "/".join(jieba.cut(inputString,cut_all=False))
    
if __name__ == '__main__':
    ser_input=raw_input('please say something:')
    print (ser_input)
    result=TokenWord(ser_input)
    
    #result.replace(",|.|？","/")  #用'/'替换掉标点符号  py2没实现该步骤
    input_list=result.split('/')
    for i in input_list:
        print(i)
        if i in string.punctuation:
            print(i)
            input_list.pop(input_list.index(i))
    for i in input_list: 
        print(i)
    print(input_list)

    #result = jieba.tokenize(ser_input.decode('utf-8'))  #其结果是生成器 py3环境下比较使用
    #for i in result:
    #    print(i)
 
    #xx=ReadSensitiveFile()
    #print(xx)
    print('hello')
    
    
    
    
    
    
    
    
    
    
    
    
